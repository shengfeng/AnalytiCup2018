{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import collections\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, metric, nd\n",
    "from mxnet.gluon import loss as gloss, nn, rnn\n",
    "from mxnet.contrib import text\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import time\n",
    "from time import strftime\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from spacy.lemmatizer import Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_PATH  = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noation_replace(line):\n",
    "    notations = ['?', '¿', ',', '.', '¡','!', ':', \",\", ';', '-', '']\n",
    "    output = line.strip()\n",
    "    for i in notations:\n",
    "        output = output.replace(i, '')\n",
    "    return output.split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es data size: 20000\n",
      "se data size: 1400\n",
      "test data size: 5000\n"
     ]
    }
   ],
   "source": [
    "es_e_l = []\n",
    "es_s_l = []\n",
    "es_e_r = []\n",
    "es_s_r = []\n",
    "es_labels = []\n",
    "## english-spanish text \n",
    "with open(os.path.join(ROOT_PATH, \"cikm_english_train_20180516.txt\"), 'r', encoding='utf-8') as esf:\n",
    "    for line in esf:\n",
    "        segs = noation_replace(line)\n",
    "        es_e_l.append(segs[0].lower())\n",
    "        es_e_r.append(segs[2].lower())\n",
    "        es_s_l.append(segs[1].lower())\n",
    "        es_s_r.append(segs[3].lower())\n",
    "        es_labels.append(int(segs[4]))\n",
    "        \n",
    "se_e_l = []\n",
    "se_s_l = []\n",
    "se_e_r = []\n",
    "se_s_r = []\n",
    "se_labels = []\n",
    "## spanish-english text\n",
    "with open(os.path.join(ROOT_PATH, \"cikm_spanish_train_20180516.txt\"), 'r', encoding='utf-8') as ssf:\n",
    "    for line in ssf:\n",
    "        segs = noation_replace(line)\n",
    "        se_s_l.append(segs[0].lower())\n",
    "        se_s_r.append(segs[2].lower())\n",
    "        se_e_l.append(segs[1].lower())\n",
    "        se_e_r.append(segs[3].lower())\n",
    "        se_labels.append(int(segs[4]))\n",
    "\n",
    "test_s_1 = []\n",
    "test_s_2 = []\n",
    "## spanish test file\n",
    "with open(os.path.join(ROOT_PATH, \"cikm_test_a_20180516.txt\"), 'r', encoding='utf-8') as tef:\n",
    "    for line in tef:\n",
    "        segs = noation_replace(line)\n",
    "        test_s_1.append(segs[0].lower())\n",
    "        test_s_2.append(segs[1].lower())\n",
    "\n",
    "print(\"es data size:\", len(es_s_l))\n",
    "print(\"se data size:\", len(se_e_l))\n",
    "print(\"test data size:\", len(test_s_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left data size: 21400\n",
      "right data size: 21400\n",
      "label size: 21400\n"
     ]
    }
   ],
   "source": [
    "##add data sets (s0, s1, y) + (s1, s0, y)\n",
    "left_texts = es_s_l + se_s_l\n",
    "right_texts = es_s_r + se_s_r\n",
    "y = es_labels + se_labels\n",
    "\n",
    "print(\"left data size:\", len(left_texts))\n",
    "print(\"right data size:\", len(right_texts))\n",
    "print(\"label size:\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "span_sw = []\n",
    "with open(os.path.join(ROOT_PATH, \"spanish.txt\"), 'r', encoding='utf-8') as swf:\n",
    "    for line in swf:\n",
    "        word = line.strip()\n",
    "        span_sw.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\msgpack_numpy.py:84: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  dtype=np.dtype(descr)).reshape(obj[b'shape'])\n",
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\msgpack_numpy.py:88: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  dtype=np.dtype(descr))[0]\n"
     ]
    }
   ],
   "source": [
    "spacy_en = spacy.load('en')\n",
    "spacy_es = spacy.load('es')\n",
    "\n",
    "def en_tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def es_tokenizer(text):\n",
    "    return [tok.text for tok in spacy_es.tokenizer(text) if tok.text not in span_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique token count : 5824\n"
     ]
    }
   ],
   "source": [
    "left_tokenized = []\n",
    "right_tokenized = []\n",
    "test_left_tokenized = []\n",
    "test_right_tokenized = []\n",
    "\n",
    "for token in left_texts:\n",
    "    left_tokenized.append(es_tokenizer(token))\n",
    "\n",
    "for token in right_texts:\n",
    "    right_tokenized.append(es_tokenizer(token))\n",
    "    \n",
    "for token in test_s_1:\n",
    "    test_left_tokenized.append(es_tokenizer(token))\n",
    "\n",
    "for token in test_s_2:\n",
    "    test_right_tokenized.append(es_tokenizer(token))\n",
    "    \n",
    "    \n",
    "token_counter = collections.Counter()\n",
    "for sample in left_tokenized:\n",
    "    for token in sample:\n",
    "        if token not in token_counter:\n",
    "            token_counter[token] = 1\n",
    "        else:\n",
    "            token_counter[token] += 1\n",
    "            \n",
    "for sample in right_tokenized:\n",
    "    for token in sample:\n",
    "        if token not in token_counter:\n",
    "            token_counter[token] = 1\n",
    "        else:\n",
    "            token_counter[token] += 1\n",
    "            \n",
    "for sample in test_left_tokenized:\n",
    "    for token in sample:\n",
    "        if token not in token_counter:\n",
    "            token_counter[token] = 1\n",
    "        else:\n",
    "            token_counter[token] += 1\n",
    "\n",
    "for sample in test_right_tokenized:\n",
    "    for token in sample:\n",
    "        if token not in token_counter:\n",
    "            token_counter[token] = 1\n",
    "        else:\n",
    "            token_counter[token] += 1\n",
    "            \n",
    "print(\"unique token count :\", len(token_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 根据词典，将数据转换成特征向量。\n",
    "def encode_samples(tokenized_samples, vocab):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in vocab.token_to_idx:\n",
    "                feature.append(vocab.token_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)         \n",
    "    return features\n",
    "\n",
    "def pad_samples(features, maxlen=500, padding=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) > maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            # 添加 PAD 符号使每个序列等长（长度为 maxlen ）。\n",
    "            while len(padded_feature) < maxlen:\n",
    "                padded_feature.append(padding)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = text.vocab.Vocabulary(token_counter, unknown_token='<unk>', reserved_tokens=None)\n",
    "\n",
    "left_texts_features = encode_samples(left_tokenized, vocab)\n",
    "right_texts_featrues = encode_samples(right_tokenized, vocab)\n",
    "\n",
    "left_padded_features = pad_samples(left_texts_features, 35, 0)\n",
    "right_texts_featrues = pad_samples(right_texts_featrues, 35, 0)\n",
    "\n",
    "test_left_features = encode_samples(test_left_tokenized, vocab)\n",
    "test_right_featrues = encode_samples(test_right_tokenized, vocab)\n",
    "\n",
    "test_left_padded_features = pad_samples(test_left_features, 35, 0)\n",
    "test_right_texts_featrues = pad_samples(test_right_featrues, 35, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_validation_samples = int(0.2 * len(left_padded_features))\n",
    "\n",
    "left_x_train = left_padded_features[:-num_validation_samples]\n",
    "left_x_val = left_padded_features[-num_validation_samples:]\n",
    "\n",
    "right_x_train = right_texts_featrues[:-num_validation_samples]\n",
    "right_x_val = right_texts_featrues[-num_validation_samples:]\n",
    "\n",
    "y_train = y[:-num_validation_samples]\n",
    "y_val = y[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_inputs = []\n",
    "for i in range(len(left_x_train)):\n",
    "    train_inputs.append([left_x_train[i], right_x_train[i]])\n",
    "    \n",
    "val_inputs = []\n",
    "for i in range(len(left_x_val)):\n",
    "    val_inputs.append([left_x_val[i], right_x_val[i]])\n",
    "\n",
    "test_inputs = []\n",
    "for i in range(len(test_left_padded_features)):\n",
    "    test_inputs.append([test_left_padded_features[i], test_right_texts_featrues[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "\n",
    "train_features = nd.array(train_inputs, ctx=ctx)\n",
    "train_label = nd.array(y_train, ctx)\n",
    "val_features = nd.array(val_inputs, ctx=ctx)\n",
    "val_label = nd.array(y_val, ctx)\n",
    "test_featrues = nd.array(test_inputs, ctx=ctx)\n",
    "# test_label = nd.array(test_label, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\mxnet\\contrib\\text\\embedding.py:278: UserWarning: At line 1 of the pre-trained text embedding file: token 985667 with 1-dimensional vector [300.0] is likely a header and is skipped.\n",
      "  'skipped.' % (line_num, token, elems))\n"
     ]
    }
   ],
   "source": [
    "glove_embedding = text.embedding.CustomEmbedding(pretrained_file_path='data/wiki.es.vec', vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#相似度计算\n",
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    output = nd.exp(-nd.sum(nd.abs(left - right), axis=1, keepdims=True))\n",
    "#     print('the shape of left: ', left.shape)\n",
    "#     print('the shape of right: ', right.shape)\n",
    "#     print('the shape of output: ', output.shape)\n",
    "    return output\n",
    "\n",
    "def cos_distance(left, right):\n",
    "    top = nd.sum(left * right, axis=1)\n",
    "    normL = nd.sum(left ** 2, axis=1)\n",
    "    normR = nd.sum(right ** 2, axis=1)\n",
    "    output = top / ((normL ** 0.5) * (normR ** 0.5))\n",
    "#     print('the shape of output: ', output.shape)\n",
    "#     print(output)\n",
    "    return output.reshape(100, 1)\n",
    "    \n",
    "\n",
    "# new metric\n",
    "def logloss(y_true, y_pred):\n",
    "    return -np.mean(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,\n",
    "             legend=None, figsize=(3.5, 2.5)):\n",
    "    gb.set_figsize(figsize)\n",
    "    gb.plt.xlabel(x_label)\n",
    "    gb.plt.ylabel(y_label)\n",
    "    gb.plt.semilogy(x_vals, y_vals)\n",
    "    if x2_vals and y2_vals:\n",
    "        gb.plt.semilogy(x2_vals, y2_vals)\n",
    "        gb.plt.legend(legend)\n",
    "    gb.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nin_block(num_channels):\n",
    "    blk = nn.Sequential()\n",
    "    blk.add(nn.Conv2D(num_channels, kernel_size=1, activation='relu'),\n",
    "            nn.Conv2D(num_channels, kernel_size=1, activation='relu'))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Block):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers,\n",
    "                 bidirectional, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "                \n",
    "#             self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers,\n",
    "#                                     bidirectional=bidirectional,\n",
    "#                                     input_size=embed_size)\n",
    "            self.encoder = rnn.GRU(num_hiddens, num_layers, bidirectional=bidirectional, \n",
    "                                   input_size=embed_size, dropout=0.2)\n",
    "            self.dense0 = nn.Dense(100, activation='relu', flatten=False)\n",
    "#             self.dense0 = nin_block(100)\n",
    "            self.dropout1 = nn.Dropout(0.5)\n",
    "            self.norm2 = nn.BatchNorm()\n",
    "#             self.dense1 = nin_block(50)\n",
    "            self.dense1 = nn.Dense(50, activation='relu', flatten=False)\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "            self.norm3 = nn.BatchNorm()\n",
    "            self.decoder = nn.Dense(num_outputs, flatten=False, activation='sigmoid')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = nd.transpose(inputs, axes=(1, 2, 0))\n",
    "        \n",
    "        bembeddings = self.embedding(inputs[0])\n",
    "        bstates = self.encoder(bembeddings)\n",
    "        # 连结初始时间步和最终时间步的隐藏状态。\n",
    "        bencoding = nd.concat(bstates[0], bstates[-1])\n",
    "        bencoding = self.dense0(bencoding)\n",
    "        \n",
    "        eembeddings = self.embedding(inputs[1])\n",
    "        estates = self.encoder(eembeddings)\n",
    "        # 连结初始时间步和最终时间步的隐藏状态。\n",
    "        eencoding = nd.concat(estates[0], estates[-1])\n",
    "        eencoding = self.dense0(eencoding)\n",
    "        \n",
    "        y = nd.subtract(bencoding, eencoding)\n",
    "#         y = cos_distance(bencoding, eencoding)\n",
    "        outputs = self.dropout1(y)\n",
    "        outputs = self.norm2(outputs)\n",
    "        outputs = self.dense1(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.norm3(outputs)\n",
    "        outputs = self.decoder(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_outputs = 1\n",
    "lr = 0.1\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "embed_size = 300\n",
    "num_hiddens = 200\n",
    "num_layers = 2\n",
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SentimentNet(vocab, embed_size, num_hiddens, num_layers, bidirectional)\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n",
    "# 设置 embedding 层的 weight 为预训练的词向量。\n",
    "net.embedding.weight.set_data(glove_embedding.idx_to_vec.as_in_context(ctx))\n",
    "# 训练中不更新词向量（net.embedding中的模型参数）。\n",
    "net.embedding.collect_params().setattr('grad_req', 'null')\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "loss = gloss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_online_1 = []\n",
    "test_online_2 = []\n",
    "test_label = []\n",
    "## spanish test file\n",
    "with open(os.path.join(ROOT_PATH, \"test_300_label.txt\"), 'r', encoding='utf-8') as tef:\n",
    "    for line in tef:\n",
    "        segs = noation_replace(line)\n",
    "        test_online_1.append(segs[0].lower())\n",
    "        test_online_2.append(segs[1].lower())\n",
    "        test_label.append(segs[2].lower())\n",
    "\n",
    "test_online_left_tokenized = []\n",
    "test_online_right_tokenized = []\n",
    "\n",
    "for token in test_online_1:\n",
    "    test_online_left_tokenized.append(es_tokenizer(token))\n",
    "\n",
    "for token in test_online_2:\n",
    "    test_online_right_tokenized.append(es_tokenizer(token))\n",
    "\n",
    "test_online_left_features = encode_samples(test_online_left_tokenized, vocab)\n",
    "test_online_right_featrues = encode_samples(test_online_right_tokenized, vocab)\n",
    "\n",
    "test_online_left_padded_features = pad_samples(test_online_left_features, 40, 0)\n",
    "test_online_right_texts_featrues = pad_samples(test_online_right_featrues, 40, 0)\n",
    "\n",
    "test_online_inputs = []\n",
    "for i in range(len(test_online_left_padded_features)):\n",
    "    test_online_inputs.append([test_online_left_padded_features[i], test_online_right_texts_featrues[i]])\n",
    "\n",
    "test_online_featrues = nd.array(test_online_inputs, ctx=ctx)\n",
    "test_online_label= nd.array(test_label, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(features, labels):\n",
    "    l_sum = 0\n",
    "    for i in range(features.shape[0] // batch_size):\n",
    "        X = features[i*batch_size : (i+1)*batch_size].as_in_context(ctx)\n",
    "        y = labels[i*batch_size :(i+1)*batch_size].as_in_context(ctx)\n",
    "        output = net(X)\n",
    "        l = loss(output, y)\n",
    "        l_sum += l.sum().asscalar() \n",
    "    return l_sum / features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 0.544655, val loss 0.511275, test_loss 0.652183\n",
      "epoch 2, train loss 0.527403, val loss 0.498415, test_loss 0.631198\n",
      "epoch 3, train loss 0.508451, val loss 0.486533, test_loss 0.604716\n",
      "epoch 4, train loss 0.492175, val loss 0.475984, test_loss 0.583239\n",
      "epoch 5, train loss 0.471631, val loss 0.462152, test_loss 0.569079\n",
      "epoch 6, train loss 0.460753, val loss 0.455670, test_loss 0.548176\n",
      "epoch 7, train loss 0.449730, val loss 0.449374, test_loss 0.548253\n",
      "epoch 8, train loss 0.429281, val loss 0.437299, test_loss 0.544887\n",
      "epoch 9, train loss 0.420902, val loss 0.440126, test_loss 0.544642\n",
      "epoch 10, train loss 0.405040, val loss 0.427196, test_loss 0.544019\n",
      "epoch 11, train loss 0.397702, val loss 0.428325, test_loss 0.538214\n",
      "epoch 12, train loss 0.388908, val loss 0.424789, test_loss 0.533082\n",
      "epoch 13, train loss 0.379741, val loss 0.421097, test_loss 0.535190\n",
      "epoch 14, train loss 0.376968, val loss 0.419293, test_loss 0.530266\n",
      "epoch 15, train loss 0.355802, val loss 0.403001, test_loss 0.531323\n",
      "epoch 16, train loss 0.353045, val loss 0.407073, test_loss 0.535837\n",
      "epoch 17, train loss 0.344472, val loss 0.405953, test_loss 0.533414\n",
      "epoch 18, train loss 0.345217, val loss 0.404890, test_loss 0.542599\n",
      "epoch 19, train loss 0.335126, val loss 0.404048, test_loss 0.534061\n",
      "epoch 20, train loss 0.323326, val loss 0.396321, test_loss 0.523381\n",
      "epoch 21, train loss 0.324583, val loss 0.403768, test_loss 0.532989\n",
      "epoch 22, train loss 0.317702, val loss 0.395396, test_loss 0.525584\n",
      "epoch 23, train loss 0.312733, val loss 0.397003, test_loss 0.521418\n",
      "epoch 24, train loss 0.309642, val loss 0.393822, test_loss 0.522460\n",
      "epoch 25, train loss 0.320151, val loss 0.401620, test_loss 0.531386\n",
      "epoch 26, train loss 0.306246, val loss 0.393938, test_loss 0.550751\n",
      "epoch 27, train loss 0.304630, val loss 0.394461, test_loss 0.548732\n",
      "epoch 28, train loss 0.306195, val loss 0.399395, test_loss 0.537771\n",
      "epoch 29, train loss 0.293511, val loss 0.394924, test_loss 0.526115\n",
      "epoch 30, train loss 0.327732, val loss 0.411452, test_loss 0.549430\n",
      "epoch 31, train loss 0.355571, val loss 0.426776, test_loss 0.549037\n",
      "epoch 32, train loss 0.326623, val loss 0.406163, test_loss 0.538095\n",
      "epoch 33, train loss 0.300013, val loss 0.396538, test_loss 0.532320\n",
      "epoch 34, train loss 0.296358, val loss 0.403629, test_loss 0.530947\n",
      "epoch 35, train loss 0.293060, val loss 0.403641, test_loss 0.546684\n",
      "epoch 36, train loss 0.293836, val loss 0.408792, test_loss 0.547439\n",
      "epoch 37, train loss 0.305417, val loss 0.413196, test_loss 0.546654\n",
      "epoch 38, train loss 0.286528, val loss 0.405209, test_loss 0.531057\n",
      "epoch 39, train loss 0.268205, val loss 0.394219, test_loss 0.551908\n",
      "epoch 40, train loss 0.263737, val loss 0.397316, test_loss 0.519646\n",
      "epoch 41, train loss 0.278367, val loss 0.403492, test_loss 0.542289\n",
      "epoch 42, train loss 0.295680, val loss 0.412829, test_loss 0.570475\n",
      "epoch 43, train loss 0.334402, val loss 0.435890, test_loss 0.565126\n",
      "epoch 44, train loss 0.275550, val loss 0.404504, test_loss 0.543952\n",
      "epoch 45, train loss 0.285748, val loss 0.418444, test_loss 0.541602\n",
      "epoch 46, train loss 0.254383, val loss 0.396494, test_loss 0.548023\n",
      "epoch 47, train loss 0.263872, val loss 0.409259, test_loss 0.534542\n",
      "epoch 48, train loss 0.258379, val loss 0.406121, test_loss 0.533056\n",
      "epoch 49, train loss 0.244825, val loss 0.400530, test_loss 0.536559\n",
      "epoch 50, train loss 0.239992, val loss 0.399017, test_loss 0.543590\n",
      "epoch 51, train loss 0.239934, val loss 0.407316, test_loss 0.536642\n",
      "epoch 52, train loss 0.232872, val loss 0.397489, test_loss 0.538525\n",
      "epoch 53, train loss 0.238614, val loss 0.395987, test_loss 0.528319\n",
      "epoch 54, train loss 0.244355, val loss 0.404721, test_loss 0.554778\n",
      "epoch 55, train loss 0.464882, val loss 0.565545, test_loss 0.625272\n",
      "epoch 56, train loss 0.240582, val loss 0.409397, test_loss 0.535482\n",
      "epoch 57, train loss 0.253147, val loss 0.413279, test_loss 0.551677\n",
      "epoch 58, train loss 0.242630, val loss 0.420881, test_loss 0.592545\n",
      "epoch 59, train loss 0.252505, val loss 0.410437, test_loss 0.551268\n",
      "epoch 60, train loss 0.235258, val loss 0.403447, test_loss 0.546117\n",
      "epoch 61, train loss 0.237437, val loss 0.408227, test_loss 0.517390\n",
      "epoch 62, train loss 0.216366, val loss 0.408732, test_loss 0.539719\n",
      "epoch 63, train loss 0.228514, val loss 0.408745, test_loss 0.565458\n",
      "epoch 64, train loss 0.213296, val loss 0.405312, test_loss 0.557326\n",
      "epoch 65, train loss 0.217583, val loss 0.402121, test_loss 0.557900\n",
      "epoch 66, train loss 0.219331, val loss 0.411336, test_loss 0.586753\n",
      "epoch 67, train loss 0.203235, val loss 0.399879, test_loss 0.573144\n",
      "epoch 68, train loss 0.207727, val loss 0.401905, test_loss 0.576953\n",
      "epoch 69, train loss 0.203530, val loss 0.411928, test_loss 0.591329\n",
      "epoch 70, train loss 0.193523, val loss 0.411269, test_loss 0.641609\n",
      "epoch 71, train loss 0.200148, val loss 0.410742, test_loss 0.587153\n",
      "epoch 72, train loss 0.195737, val loss 0.411218, test_loss 0.592720\n",
      "epoch 73, train loss 0.198464, val loss 0.426484, test_loss 0.602676\n",
      "epoch 74, train loss 0.182878, val loss 0.425936, test_loss 0.636732\n",
      "epoch 75, train loss 0.178600, val loss 0.411547, test_loss 0.611336\n",
      "epoch 76, train loss 0.185959, val loss 0.426802, test_loss 0.599089\n",
      "epoch 77, train loss 0.176078, val loss 0.417989, test_loss 0.617161\n",
      "epoch 78, train loss 0.179276, val loss 0.420329, test_loss 0.626855\n",
      "epoch 79, train loss 0.189873, val loss 0.423788, test_loss 0.626638\n",
      "epoch 80, train loss 0.176329, val loss 0.425245, test_loss 0.612283\n",
      "epoch 81, train loss 0.171464, val loss 0.417185, test_loss 0.629196\n",
      "epoch 82, train loss 0.168251, val loss 0.416277, test_loss 0.617566\n",
      "epoch 83, train loss 0.174122, val loss 0.427080, test_loss 0.641624\n",
      "epoch 84, train loss 0.167866, val loss 0.428067, test_loss 0.666488\n",
      "epoch 85, train loss 0.167839, val loss 0.422786, test_loss 0.647543\n",
      "epoch 86, train loss 0.162159, val loss 0.426343, test_loss 0.649791\n",
      "epoch 87, train loss 0.156354, val loss 0.422545, test_loss 0.677457\n",
      "epoch 88, train loss 0.155330, val loss 0.425605, test_loss 0.652843\n",
      "epoch 89, train loss 0.157037, val loss 0.429864, test_loss 0.681093\n",
      "epoch 90, train loss 0.155199, val loss 0.424053, test_loss 0.673926\n",
      "epoch 91, train loss 0.155855, val loss 0.425221, test_loss 0.672605\n",
      "epoch 92, train loss 0.157053, val loss 0.441479, test_loss 0.705535\n",
      "epoch 93, train loss 0.158712, val loss 0.435398, test_loss 0.666871\n",
      "epoch 94, train loss 0.150255, val loss 0.432276, test_loss 0.692419\n",
      "epoch 95, train loss 0.153840, val loss 0.440393, test_loss 0.725755\n",
      "epoch 96, train loss 0.135903, val loss 0.437142, test_loss 0.752648\n",
      "epoch 97, train loss 0.148854, val loss 0.442949, test_loss 0.714015\n",
      "epoch 98, train loss 0.145210, val loss 0.433574, test_loss 0.696883\n",
      "epoch 99, train loss 0.152066, val loss 0.436928, test_loss 0.714016\n",
      "epoch 100, train loss 0.147378, val loss 0.448335, test_loss 0.705120\n"
     ]
    }
   ],
   "source": [
    "model_file = \"model/Model_\" + strftime(\"%Y-%m-%d %H-%M\", time.localtime()) + \".mdl\"\n",
    "best_loss = 1.0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for i in range(train_features.shape[0] // batch_size):\n",
    "        X = train_features[i * batch_size: (i + 1) * batch_size].as_in_context(ctx)\n",
    "        y = train_label[i * batch_size: (i + 1) * batch_size].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    train_loss = eval_model(train_features, train_label)\n",
    "    val_loss = eval_model(val_features, val_label)\n",
    "    test_loss = eval_model(test_online_featrues, test_online_label)\n",
    "    if val_loss < best_loss :\n",
    "        net.save_params(model_file)\n",
    "        best_loss = val_loss\n",
    "    print('epoch %d, train loss %.6f, val loss %.6f, test_loss %.6f' % (epoch, train_loss, val_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(features):\n",
    "    output = []\n",
    "    for i in range(features.shape[0] // batch_size):\n",
    "        X = features[i*batch_size : (i+1)*batch_size].as_in_context(ctx)\n",
    "        y = net(X).asnumpy().flatten()\n",
    "        for i in y:\n",
    "            output.append(i)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = predict(test_featrues)\n",
    "\n",
    "result_file_name = strftime(\"%Y-%m-%d %H-%M\", time.localtime()) + \"result.txt\"\n",
    "\n",
    "with open(result_file_name, 'w') as f:\n",
    "    for line in output:\n",
    "        f.write(str(round(line, 6)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "texts = left_tokenized + right_tokenized + test_left_tokenized + test_right_tokenized\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts if text not in span_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "tfidf.save(\"model.tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "output = []\n",
    "for doc in corpus_tfidf:\n",
    "    output.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(output[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=5, iterations=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda.save('mylda_v1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, score in sorted(lda[corpus_tfidf[0]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: %f\\t Topic: %s \\n\" % (score, lda.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
