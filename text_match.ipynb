{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Read sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from time import strftime\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.layers import Embedding, Dropout, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_PATH  = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es data size: 20000\n",
      "se data size: 1400\n",
      "test data size: 5000\n"
     ]
    }
   ],
   "source": [
    "es_e_l = []\n",
    "es_s_l = []\n",
    "es_e_r = []\n",
    "es_s_r = []\n",
    "es_labels = []\n",
    "## english-spanish text \n",
    "with open(os.path.join(ROOT_PATH, \"cikm_english_train_20180516.txt\"), 'r', encoding='utf-8') as esf:\n",
    "    for line in esf:\n",
    "        segs = line.strip().replace('?','').split('\\t')\n",
    "        es_e_l.append(segs[0].lower())\n",
    "        es_e_r.append(segs[2].lower())\n",
    "        es_s_l.append(segs[1].lower())\n",
    "        es_s_r.append(segs[3].lower())\n",
    "        es_labels.append(int(segs[4]))\n",
    "        \n",
    "se_e_l = []\n",
    "se_s_l = []\n",
    "se_e_r = []\n",
    "se_s_r = []\n",
    "se_labels = []\n",
    "## spanish-english text\n",
    "with open(os.path.join(ROOT_PATH, \"cikm_spanish_train_20180516.txt\"), 'r', encoding='utf-8') as ssf:\n",
    "    for line in ssf:\n",
    "        segs = line.strip().replace('?','').split('\\t')\n",
    "        se_e_l.append(segs[1].lower())\n",
    "        se_e_r.append(segs[3].lower())\n",
    "        se_s_l.append(segs[0].lower())\n",
    "        se_s_r.append(segs[2].lower())\n",
    "        se_labels.append(int(segs[4]))\n",
    "\n",
    "test_s_1 = []\n",
    "test_s_2 = []\n",
    "## spanish test file\n",
    "with open(os.path.join(ROOT_PATH, \"cikm_test_a_20180516.txt\"), 'r', encoding='utf-8') as tef:\n",
    "    for line in tef:\n",
    "        segs = line.strip().replace('?','').replace('¿', '').split('\\t')\n",
    "        test_s_1.append(segs[0].lower())\n",
    "        test_s_2.append(segs[1].lower())\n",
    "\n",
    "print(\"es data size:\", len(es_s_l))\n",
    "print(\"se data size:\", len(se_e_l))\n",
    "print(\"test data size:\", len(test_s_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## es.vec\n",
    "es_vec = {}\n",
    "with open(os.path.join(ROOT_PATH, \"wiki.es.vec\"), 'r', encoding='utf-8') as vecf:\n",
    "    i = 0\n",
    "    for line in vecf:\n",
    "        if i == 0:\n",
    "            continue\n",
    "        i = 1\n",
    "        segs = line.strip().split(' ')\n",
    "        es_vec[segs[0]] = map(eval, segs[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_MAX_SEQUENCE_LENGTH = 60  #左边最大句子长度\n",
    "R_MAX_SEQUENCE_LENGTH = 50  #右边最大句子长度\n",
    "MAX_NB_WORDS = 20000      #词典大小，词的个数\n",
    "EMBEDDING_DIM = 300       #词向量维度\n",
    "VALIDATION_SPLIT = 0.2    # 测试集比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left data size: 21400\n",
      "right data size: 21400\n",
      "label size: 21400\n"
     ]
    }
   ],
   "source": [
    "##add data sets (s0, s1, y) + (s1, s0, y)\n",
    "left_texts = se_s_l + es_s_l\n",
    "right_texts = se_s_r + es_s_r\n",
    "y = se_labels + es_labels\n",
    "print(\"left data size:\", len(left_texts))\n",
    "print(\"right data size:\", len(right_texts))\n",
    "print(\"label size:\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\keras_preprocessing\\text.py:174: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "Found 3286 unique tokens.\n",
      "Shape of data tensor: (21400, 60)\n",
      "Preparing embedding matrix. : (3287, 300)\n"
     ]
    }
   ],
   "source": [
    "# prepare left embedding matrix\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(left_texts)\n",
    "sequences = tokenizer.texts_to_sequences(left_texts)\n",
    "MAX_LENGTH = 0\n",
    "for s in sequences:\n",
    "    if len(s) > MAX_LENGTH:\n",
    "        MAX_LENGTH = len(s)\n",
    "print(MAX_LENGTH)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "left_data = pad_sequences(sequences, maxlen=L_MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', left_data.shape)\n",
    "\n",
    "### Prepare left embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "left_embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM))\n",
    "print('Preparing embedding matrix. :', left_embedding_matrix.shape)\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = es_vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        left_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        \n",
    "left_embedding_layer = Embedding(num_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[left_embedding_matrix],\n",
    "                            input_length=L_MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\keras_preprocessing\\text.py:174: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "Found 2787 unique tokens.\n",
      "Shape of data tensor: (21400, 50)\n",
      "Preparing embedding matrix. : (2788, 300)\n"
     ]
    }
   ],
   "source": [
    "# prepare right embedding matrix\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(right_texts)\n",
    "sequences = tokenizer.texts_to_sequences(right_texts)\n",
    "MAX_LENGTH = 0\n",
    "for s in sequences:\n",
    "    if len(s) > MAX_LENGTH:\n",
    "        MAX_LENGTH = len(s)\n",
    "print(MAX_LENGTH)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "right_data = pad_sequences(sequences, maxlen=R_MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', right_data.shape)\n",
    "\n",
    "### Prepare right embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "right_embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM))\n",
    "print('Preparing embedding matrix. :', right_embedding_matrix.shape)\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = es_vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        right_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        \n",
    "right_embedding_layer = Embedding(num_words + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[right_embedding_matrix],\n",
    "                            input_length=R_MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4280\n"
     ]
    }
   ],
   "source": [
    "## split train and val sets\n",
    "indices = np.arange(left_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = left_data[indices]\n",
    "y = np.array(y)\n",
    "labels = y[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "print(num_validation_samples)\n",
    "\n",
    "left_x_train = data[:-num_validation_samples]\n",
    "left_x_val = data[-num_validation_samples:]\n",
    "\n",
    "data = right_data[indices]\n",
    "right_x_train = data[:-num_validation_samples]\n",
    "right_x_val = data[-num_validation_samples:]\n",
    "\n",
    "y_train = labels[:-num_validation_samples]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\keras_preprocessing\\text.py:174: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3587 unique tokens.\n",
      "Shape of data tensor: (5000, 60)\n",
      "Found 1666 unique tokens.\n",
      "Shape of data tensor: (5000, 50)\n"
     ]
    }
   ],
   "source": [
    "## prepare test data\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(test_s_1)\n",
    "sequences = tokenizer.texts_to_sequences(test_s_1)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "test_1 = pad_sequences(sequences, maxlen=L_MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', test_1.shape)\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(test_s_2)\n",
    "sequences = tokenizer.texts_to_sequences(test_s_2)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "test_2 = pad_sequences(sequences, maxlen=R_MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', test_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "input1 = keras.layers.Input(shape=(L_MAX_SEQUENCE_LENGTH,), dtype='float32')\n",
    "print(input1.shape)\n",
    "left_embedded_sequences = left_embedding_layer(input1)\n",
    "x1 = LSTM(128, dropout_W=0.2, dropout_U=0.2)(left_embedded_sequences)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "#x1 = BatchNormalization()(x1)\n",
    "x1 = Dense(64, activation='relu')(x1) ## acitivation = tanh, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "input2 = keras.layers.Input(shape=(R_MAX_SEQUENCE_LENGTH,), dtype='float32')\n",
    "print(input2.shape)\n",
    "right_embedded_sequences = right_embedding_layer(input2)\n",
    "x2 = LSTM(128, dropout_W=0.2, dropout_U=0.2)(right_embedded_sequences)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "#x1 = BatchNormalization()(x1)\n",
    "x2 = Dense(64, activation='relu')(x2) ## acitivation = tanh, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new metric\n",
    "def logloss(y_true, y_pred):\n",
    "    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17120 samples, validate on 4280 samples\n",
      "Epoch 1/20\n",
      "17120/17120 [==============================] - 53s 3ms/step - loss: 0.5920 - logloss: 0.5920 - val_loss: 0.5688 - val_logloss: 0.5688\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\keras\\callbacks.py:535: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_logloss,loss,logloss\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\keras\\callbacks.py:432: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1900/17120 [==>...........................] - ETA: 41s - loss: 0.5543 - logloss: 0.5543"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-54d51e32638e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlogloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m## optimizer= sgd, adam, rmsprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mleft_x_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_x_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mleft_x_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_x_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mpredicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "merged = keras.layers.add([x1, x2])  # add, concatenate, maximum\n",
    "# We stack a deep densely-connected network on top\n",
    "#merged = Conv1D(filters=50, kernel_size=5, activation='relu')(merged)\n",
    "#merged = MaxPooling1D(pool_size=5)(merged)\n",
    "#merged = Flatten()(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(32, activation='tanh')(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "model_file = \"Model_\" + strftime(\"%Y-%m-%d %H-%M\", time.localtime()) + \".mdl\"\n",
    "model_checkpoint = ModelCheckpoint(model_file, monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(loss='binary_crossentropy',  optimizer='adam', metrics=[logloss])  ## optimizer= sgd, adam, rmsprop\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "model.fit([left_x_train, right_x_train], y_train, batch_size=100, nb_epoch=20, validation_data=([left_x_val, right_x_val], y_val), callbacks=[early_stopping, model_checkpoint])\n",
    "predicts = model.predict([test_1, test_2], batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[left_x_train, right_x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
