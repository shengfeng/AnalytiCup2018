{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Read sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from time import strftime\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.layers import Embedding, Dropout, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import spacy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_session(tf.Session(config=tf.ConfigProto(device_count={'gpu':0})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_PATH  = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es data size: 20000\n",
      "se data size: 1400\n",
      "test data size: 5000\n"
     ]
    }
   ],
   "source": [
    "es_e_l = []\n",
    "es_s_l = []\n",
    "es_e_r = []\n",
    "es_s_r = []\n",
    "es_labels = []\n",
    "## english-spanish text \n",
    "with open(os.path.join(ROOT_PATH, \"data/cikm_english_train_20180516.txt\"), 'r', encoding='utf-8') as esf:\n",
    "    for line in esf:\n",
    "        segs = line.strip().replace('?','').replace('¿', '').replace(',', '').replace('.', '').split('\\t')\n",
    "        es_e_l.append(segs[0].lower())\n",
    "        es_e_r.append(segs[2].lower())\n",
    "        es_s_l.append(segs[1].lower())\n",
    "        es_s_r.append(segs[3].lower())\n",
    "        es_labels.append(int(segs[4]))\n",
    "        \n",
    "se_e_l = []\n",
    "se_s_l = []\n",
    "se_e_r = []\n",
    "se_s_r = []\n",
    "se_labels = []\n",
    "## spanish-english text\n",
    "with open(os.path.join(ROOT_PATH, \"data/cikm_spanish_train_20180516.txt\"), 'r', encoding='utf-8') as ssf:\n",
    "    for line in ssf:\n",
    "        segs = line.strip().replace('?','').replace('¿', '').replace(',', '').replace('.', '').split('\\t')\n",
    "        se_s_l.append(segs[0].lower())\n",
    "        se_s_r.append(segs[2].lower())\n",
    "        se_e_l.append(segs[1].lower())\n",
    "        se_e_r.append(segs[3].lower())\n",
    "        se_labels.append(int(segs[4]))\n",
    "\n",
    "test_s_1 = []\n",
    "test_s_2 = []\n",
    "## spanish test file\n",
    "with open(os.path.join(ROOT_PATH, \"data/cikm_test_a_20180516.txt\"), 'r', encoding='utf-8') as tef:\n",
    "    for line in tef:\n",
    "        segs = line.strip().replace('?','').replace('¿', '').replace(',', '').replace('.', '').split('\\t')\n",
    "        test_s_1.append(segs[0].lower())\n",
    "        test_s_2.append(segs[1].lower())\n",
    "\n",
    "print(\"es data size:\", len(es_s_l))\n",
    "print(\"se data size:\", len(se_e_l))\n",
    "print(\"test data size:\", len(test_s_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left data size: 21400\n",
      "right data size: 21400\n",
      "label size: 21400\n"
     ]
    }
   ],
   "source": [
    "##add data sets (s0, s1, y) + (s1, s0, y)\n",
    "left_texts = es_s_l + se_s_l \n",
    "right_texts = es_s_r + se_s_r\n",
    "y = es_labels + se_labels\n",
    "#left_texts += se_s_r + es_s_r\n",
    "#right_texts += es_s_l + se_s_l \n",
    "#y += se_labels + es_labels\n",
    "print(\"left data size:\", len(left_texts))\n",
    "print(\"right data size:\", len(right_texts))\n",
    "print(\"label size:\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "span_sw = []\n",
    "with open(os.path.join(ROOT_PATH, \"data/spanish.txt\"), 'r', encoding='utf-8') as swf:\n",
    "    for line in swf:\n",
    "        word = line.strip()\n",
    "        span_sw.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_es = spacy.load('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def es_tokenizer(text):\n",
    "    return [tok.lemma_ for tok in spacy_es.tokenizer(text) if tok.text not in span_sw ]\n",
    "\n",
    "def texts_to_sequences(tokenized_texts, token_counter):\n",
    "    sequences = []\n",
    "    for texts in tokenized_texts:\n",
    "        sub_seq = []\n",
    "        for token in texts:\n",
    "            sub_seq.append(token_counter[token])\n",
    "        sequences.append(sub_seq)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique token count:  3785\n"
     ]
    }
   ],
   "source": [
    "## train text data\n",
    "left_tokenized = []\n",
    "right_tokenized = []\n",
    "## test text data\n",
    "test_left_tokenized = []\n",
    "test_right_tokenized = []\n",
    "\n",
    "for token in left_texts:\n",
    "    left_tokenized.append(es_tokenizer(token))\n",
    "\n",
    "for token in right_texts:\n",
    "    right_tokenized.append(es_tokenizer(token))\n",
    "\n",
    "for token in test_s_1:\n",
    "    test_left_tokenized.append(es_tokenizer(token))\n",
    "\n",
    "for token in test_s_2:\n",
    "    test_right_tokenized.append(es_tokenizer(token))\n",
    "\n",
    "## form wrods dictionary {word: index} \n",
    "token_counter = {}\n",
    "index = 0\n",
    "for sample in left_tokenized:\n",
    "    for token in sample:\n",
    "        if token not in token_counter:\n",
    "            token_counter[token] = index\n",
    "            index += 1\n",
    "\n",
    "for sample in right_tokenized:\n",
    "    for token in sample:\n",
    "        if token not in token_counter:\n",
    "            token_counter[token] = index\n",
    "            index += 1\n",
    "\n",
    "for sample in test_left_tokenized:\n",
    "    for token in sample:\n",
    "        if token not in token_counter:\n",
    "            token_counter[token] = index\n",
    "            index += 1\n",
    "            \n",
    "for sample in test_right_tokenized:\n",
    "    for token in sample:\n",
    "        if token not in token_counter:\n",
    "            token_counter[token] = index\n",
    "            index += 1\n",
    "print(\"unique token count: \", len(token_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## es.vec\n",
    "es_vec = {}\n",
    "with open(os.path.join(ROOT_PATH, \"data/wiki.es.vec\"), 'r', encoding='utf-8') as vecf:\n",
    "    i = 0\n",
    "    for line in vecf:\n",
    "        if i == 0:\n",
    "            i = 1\n",
    "            continue\n",
    "        segs = line.strip().split(' ')\n",
    "        es_vec[segs[0]] = ' '.join(segs[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_MAX_SEQUENCE_LENGTH = 40  #左边最大句子长度\n",
    "R_MAX_SEQUENCE_LENGTH = 40  #右边最大句子长度\n",
    "MAX_SEQUENCE_LENGTH = 40    #平均最大句子长度\n",
    "MAX_NB_WORDS = 20000      #词典大小，词的个数\n",
    "EMBEDDING_DIM = 300       #词向量维度\n",
    "VALIDATION_SPLIT = 0.2    # 测试集比例\n",
    "BATCH_SIZE = 100      ##batch大小\n",
    "EPOCH = 60  ## 迭代次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Fit tokenizer\n",
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "# tokenizer.fit_on_texts(left_texts + right_texts + test_s_1 + test_s_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3785 unique tokens.\n",
      "Preparing embedding matrix. : (3786, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "#word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(token_counter))\n",
    "num_words = min(MAX_NB_WORDS, len(token_counter))\n",
    "embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM))\n",
    "print('Preparing embedding matrix. :', embedding_matrix.shape)\n",
    "for word, i in token_counter.items():\n",
    "    embedding_vector = es_vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = list(map(eval, embedding_vector.split(' ')))\n",
    "del es_vec\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "Shape of left_data tensor: (21400, 40)\n"
     ]
    }
   ],
   "source": [
    "# prepare left data\n",
    "# sequences_l = tokenizer.texts_to_sequences(left_texts)\n",
    "sequences_l = texts_to_sequences(left_tokenized, token_counter)\n",
    "MAX_LENGTH = 0\n",
    "for s in sequences_l:\n",
    "    if len(s) > MAX_LENGTH:\n",
    "        MAX_LENGTH = len(s)\n",
    "print(MAX_LENGTH)\n",
    "left_data = pad_sequences(sequences_l, maxlen=L_MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of left_data tensor:', left_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Shape of right_data tensor: (21400, 40)\n"
     ]
    }
   ],
   "source": [
    "# prepare right data\n",
    "# sequences_r = tokenizer.texts_to_sequences(right_texts)\n",
    "sequences_r = texts_to_sequences(right_tokenized, token_counter)\n",
    "MAX_LENGTH = 0\n",
    "for s in sequences_r:\n",
    "    if len(s) > MAX_LENGTH:\n",
    "        MAX_LENGTH = len(s)\n",
    "print(MAX_LENGTH)\n",
    "right_data = pad_sequences(sequences_r, maxlen=R_MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of right_data tensor:', right_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4280\n"
     ]
    }
   ],
   "source": [
    "## split train and val sets\n",
    "random.seed(7)\n",
    "indices = np.arange(left_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data_1 = left_data[indices]\n",
    "y = np.array(y)\n",
    "labels = y[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data_1.shape[0])\n",
    "print(num_validation_samples)\n",
    "\n",
    "left_x_train = data_1[:-num_validation_samples]\n",
    "left_x_val = data_1[-num_validation_samples:]\n",
    "\n",
    "data_2 = right_data[indices]\n",
    "right_x_train = data_2[:-num_validation_samples]\n",
    "right_x_val = data_2[-num_validation_samples:]\n",
    "\n",
    "y_train = labels[:-num_validation_samples]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (5000, 40)\n",
      "Shape of data tensor: (5000, 40)\n"
     ]
    }
   ],
   "source": [
    "## prepare test data\n",
    "sequences_test_1 = texts_to_sequences(test_left_tokenized, token_counter)\n",
    "test_1 = pad_sequences(sequences_test_1, maxlen=L_MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', test_1.shape)\n",
    "\n",
    "sequences_test_2 = texts_to_sequences(test_right_tokenized, token_counter)\n",
    "test_2 = pad_sequences(sequences_test_2, maxlen=R_MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', test_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import custom test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_new_1 = []\n",
    "test_new_2 = []\n",
    "label_300 = []\n",
    "## spanish test file\n",
    "with open(os.path.join(ROOT_PATH, \"data/test_300_label.txt\"), 'r', encoding='utf-8') as newf:\n",
    "    for line in newf:\n",
    "        segs = line.strip().replace('?','').replace('¿', '').replace(',', '').replace('.', '').split('\\t')\n",
    "        test_new_1.append(segs[0].lower())\n",
    "        test_new_2.append(segs[1].lower())\n",
    "        label_300.append(int(segs[2]))\n",
    "label_300 = np.array(label_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_300_left_tokenized = []\n",
    "test_300_right_tokenized = []\n",
    "for token in test_new_1:\n",
    "    test_300_left_tokenized.append(es_tokenizer(token))\n",
    "\n",
    "for token in test_new_2:\n",
    "    test_300_right_tokenized.append(es_tokenizer(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cómo', 'poder', 'recibir', 'reembolsar', 'mediante', 'tarjeta']\n",
      "['cómo', 'poder', 'recibir', 'reembolsar']\n"
     ]
    }
   ],
   "source": [
    "print(test_300_left_tokenized[0])\n",
    "print(test_300_right_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (300, 40)\n",
      "Shape of data tensor: (300, 40)\n"
     ]
    }
   ],
   "source": [
    "## prepare test 300 data\n",
    "sequences_test_300_1 = texts_to_sequences(test_300_left_tokenized, token_counter)\n",
    "test_300_1 = pad_sequences(sequences_test_300_1, maxlen=L_MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', test_300_1.shape)\n",
    "\n",
    "sequences_test_300_2 = texts_to_sequences(test_300_right_tokenized, token_counter)\n",
    "test_300_2 = pad_sequences(sequences_test_300_2, maxlen=R_MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', test_300_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_layer = Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2), merge_mode='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input1 = keras.layers.Input(shape=(L_MAX_SEQUENCE_LENGTH,), dtype='float32')\n",
    "left_embedded_sequences = embedding_layer(input1)\n",
    "x1 = lstm_layer(left_embedded_sequences)\n",
    "#x1 = Dropout(0.5)(x1)\n",
    "#x1 = BatchNormalization()(x1)\n",
    "#x1 = Dense(64, activation='relu')(x1) ## acitivation = tanh, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input2 = keras.layers.Input(shape=(R_MAX_SEQUENCE_LENGTH,), dtype='float32')\n",
    "right_embedded_sequences = embedding_layer(input2)\n",
    "x2 = lstm_layer(right_embedded_sequences)\n",
    "#x2 = Dropout(0.5)(x2)\n",
    "#x1 = BatchNormalization()(x1)\n",
    "#x2 = Dense(64, activation='relu')(x2) ## acitivation = tanh, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new metric\n",
    "def logloss(y_true, y_pred):\n",
    "    return -K.mean(y_true*K.log(y_pred) + (1-y_true)*K.log(1-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = keras.layers.subtract([x1, x2])  # add, concatenate, maximum\n",
    "merged = Dropout(0.5)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(100, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(50, activation='relu')(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "model_file = \"Model_\" + strftime(\"%Y-%m-%d %H-%M\", time.localtime()) + \".mdl\"\n",
    "model_checkpoint = ModelCheckpoint(model_file, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(loss='binary_crossentropy',  optimizer='sgd', metrics=[logloss])  ## optimizer= sgd, adam, rmsprop\n",
    "# early_stopping = EarlyStopping(monitor='val_loss ', patience=10, mode='min')\n",
    "model.fit([left_x_train, right_x_train], y_train, batch_size=BATCH_SIZE, epochs=EPOCH, validation_data=([test_300_1, test_300_2], label_300),  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - ETA:  - ETA:  - 0s 1ms/step\n",
      "5000/5000 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 7s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate([test_300_1, test_300_2], label_300, batch_size=BATCH_SIZE, verbose=1)\n",
    "predicts = model.predict([test_1, test_2], batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 7s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "test_predicts = model.predict([test_1, test_2], batch_size=BATCH_SIZE, verbose=1)\n",
    "re_f = open('data/result_0.500.txt', 'w')\n",
    "for i in range(len(test_1)):\n",
    "    pre = predicts[i][0]\n",
    "    re_f.write(str(round(pre, 6)) + '\\n')\n",
    "re_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def cal_log_loss(y, p):\n",
    "    result = 0\n",
    "    for i in range(len(y)):\n",
    "        result += y[i]*math.log(p[i]) + (1-y[i])*math.log(1-p[i])\n",
    "    return -1 * result / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - ETA:  - ETA:  - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predicts_300 = model.predict([test_300_1, test_300_2], batch_size=BATCH_SIZE, verbose=1)\n",
    "p = []\n",
    "for i in range(len(predicts_300)):\n",
    "    pre = predicts_300[i][0]\n",
    "    p.append(round(pre, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5523937023609781"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss = cal_log_loss(label_300, p)\n",
    "log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'Model_2018-07-17 02-15.mdl', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-9e23239505f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model_2018-07-17 02-15.mdl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'logloss'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlogloss\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'Model_2018-07-17 02-15.mdl', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('Model_2018-07-17 02-15.mdl', {'logloss': logloss})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
