{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:20: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import collections\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, metric, nd\n",
    "from mxnet.gluon import loss as gloss, nn, rnn\n",
    "from mxnet.contrib import text\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import time\n",
    "from time import strftime\n",
    "from gensim import corpora\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_PATH  = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noation_replace(line):\n",
    "    notations = ['?', '¿', ',', '.', '¡','!', ':', \",\", ';', '-', '']\n",
    "    output = line.strip()\n",
    "    for i in notations:\n",
    "        output = output.replace(i, '')\n",
    "    return output.split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es data size: 20000\n",
      "se data size: 1400\n",
      "test data size: 5000\n"
     ]
    }
   ],
   "source": [
    "es_e_l = []\n",
    "es_s_l = []\n",
    "es_e_r = []\n",
    "es_s_r = []\n",
    "es_labels = []\n",
    "## english-spanish text \n",
    "with open(os.path.join(ROOT_PATH, \"cikm_english_train_20180516.txt\"), 'r', encoding='utf-8') as esf:\n",
    "    for line in esf:\n",
    "        segs = noation_replace(line)\n",
    "        es_e_l.append(segs[0].lower())\n",
    "        es_e_r.append(segs[2].lower())\n",
    "        es_s_l.append(segs[1].lower())\n",
    "        es_s_r.append(segs[3].lower())\n",
    "        es_labels.append(int(segs[4]))\n",
    "        \n",
    "se_e_l = []\n",
    "se_s_l = []\n",
    "se_e_r = []\n",
    "se_s_r = []\n",
    "se_labels = []\n",
    "## spanish-english text\n",
    "with open(os.path.join(ROOT_PATH, \"cikm_spanish_train_20180516.txt\"), 'r', encoding='utf-8') as ssf:\n",
    "    for line in ssf:\n",
    "        segs = noation_replace(line)\n",
    "        se_s_l.append(segs[0].lower())\n",
    "        se_s_r.append(segs[2].lower())\n",
    "        se_e_l.append(segs[1].lower())\n",
    "        se_e_r.append(segs[3].lower())\n",
    "        se_labels.append(int(segs[4]))\n",
    "\n",
    "test_s_1 = []\n",
    "test_s_2 = []\n",
    "## spanish test file\n",
    "with open(os.path.join(ROOT_PATH, \"cikm_test_a_20180516.txt\"), 'r', encoding='utf-8') as tef:\n",
    "    for line in tef:\n",
    "        segs = noation_replace(line)\n",
    "        test_s_1.append(segs[0].lower())\n",
    "        test_s_2.append(segs[1].lower())\n",
    "\n",
    "print(\"es data size:\", len(es_s_l))\n",
    "print(\"se data size:\", len(se_e_l))\n",
    "print(\"test data size:\", len(test_s_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left data size: 21400\n",
      "right data size: 21400\n",
      "label size: 21400\n"
     ]
    }
   ],
   "source": [
    "##add data sets (s0, s1, y) + (s1, s0, y)\n",
    "left_texts = es_s_l + se_s_l\n",
    "right_texts = es_s_r + se_s_r\n",
    "y = es_labels + se_labels\n",
    "\n",
    "print(\"left data size:\", len(left_texts))\n",
    "print(\"right data size:\", len(right_texts))\n",
    "print(\"label size:\", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "span_sw = []\n",
    "with open(os.path.join(ROOT_PATH, \"spanish.txt\"), 'r', encoding='utf-8') as swf:\n",
    "    for line in swf:\n",
    "        word = line.strip()\n",
    "        span_sw.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\msgpack_numpy.py:84: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  dtype=np.dtype(descr)).reshape(obj[b'shape'])\n",
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\msgpack_numpy.py:88: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  dtype=np.dtype(descr))[0]\n"
     ]
    }
   ],
   "source": [
    "spacy_en = spacy.load('en')\n",
    "spacy_es = spacy.load('es')\n",
    "\n",
    "def en_tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def es_tokenizer(text):\n",
    "    return [tok.text for tok in spacy_es.tokenizer(text) if tok.text not in span_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique token count : 5824\n"
     ]
    }
   ],
   "source": [
    "left_tokenized = []\n",
    "right_tokenized = []\n",
    "test_left_tokenized = []\n",
    "test_right_tokenized = []\n",
    "\n",
    "for token in left_texts:\n",
    "    left_tokenized.append(es_tokenizer(token))\n",
    "\n",
    "for token in right_texts:\n",
    "    right_tokenized.append(es_tokenizer(token))\n",
    "    \n",
    "for token in test_s_1:\n",
    "    test_left_tokenized.append(es_tokenizer(token))\n",
    "\n",
    "for token in test_s_2:\n",
    "    test_right_tokenized.append(es_tokenizer(token))\n",
    "    \n",
    "    \n",
    "token_counter = collections.Counter()\n",
    "for sample in left_tokenized:\n",
    "    for token in sample:\n",
    "        if token not in token_counter:\n",
    "            token_counter[token] = 1\n",
    "        else:\n",
    "            token_counter[token] += 1\n",
    "            \n",
    "for sample in right_tokenized:\n",
    "    for token in sample:\n",
    "        if token not in token_counter:\n",
    "            token_counter[token] = 1\n",
    "        else:\n",
    "            token_counter[token] += 1\n",
    "            \n",
    "for sample in test_left_tokenized:\n",
    "    for token in sample:\n",
    "        if token not in token_counter:\n",
    "            token_counter[token] = 1\n",
    "        else:\n",
    "            token_counter[token] += 1\n",
    "\n",
    "for sample in test_right_tokenized:\n",
    "    for token in sample:\n",
    "        if token not in token_counter:\n",
    "            token_counter[token] = 1\n",
    "        else:\n",
    "            token_counter[token] += 1\n",
    "            \n",
    "print(\"unique token count :\", len(token_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 根据词典，将数据转换成特征向量。\n",
    "def encode_samples(tokenized_samples, vocab):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in vocab.token_to_idx:\n",
    "                feature.append(vocab.token_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)         \n",
    "    return features\n",
    "\n",
    "def pad_samples(features, maxlen=500, padding=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) > maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            # 添加 PAD 符号使每个序列等长（长度为 maxlen ）。\n",
    "            while len(padded_feature) < maxlen:\n",
    "                padded_feature.append(padding)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = text.vocab.Vocabulary(token_counter, unknown_token='<unk>', reserved_tokens=None)\n",
    "\n",
    "left_texts_features = encode_samples(left_tokenized, vocab)\n",
    "right_texts_featrues = encode_samples(right_tokenized, vocab)\n",
    "\n",
    "left_padded_features = pad_samples(left_texts_features, 40, 0)\n",
    "right_texts_featrues = pad_samples(right_texts_featrues, 40, 0)\n",
    "\n",
    "test_left_features = encode_samples(test_left_tokenized, vocab)\n",
    "test_right_featrues = encode_samples(test_right_tokenized, vocab)\n",
    "\n",
    "test_left_padded_features = pad_samples(test_left_features, 40, 0)\n",
    "test_right_texts_featrues = pad_samples(test_right_featrues, 40, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_validation_samples = int(0.2 * len(left_padded_features))\n",
    "\n",
    "left_x_train = left_padded_features[:-num_validation_samples]\n",
    "left_x_val = left_padded_features[-num_validation_samples:]\n",
    "\n",
    "right_x_train = right_texts_featrues[:-num_validation_samples]\n",
    "right_x_val = right_texts_featrues[-num_validation_samples:]\n",
    "\n",
    "y_train = y[:-num_validation_samples]\n",
    "y_val = y[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_inputs = []\n",
    "for i in range(len(left_x_train)):\n",
    "    train_inputs.append([left_x_train[i], right_x_train[i]])\n",
    "    \n",
    "val_inputs = []\n",
    "for i in range(len(left_x_val)):\n",
    "    val_inputs.append([left_x_val[i], right_x_val[i]])\n",
    "\n",
    "test_inputs = []\n",
    "for i in range(len(test_left_padded_features)):\n",
    "    test_inputs.append([test_left_padded_features[i], test_right_texts_featrues[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "\n",
    "train_features = nd.array(train_inputs, ctx=ctx)\n",
    "train_label = nd.array(y_train, ctx)\n",
    "val_features = nd.array(val_inputs, ctx=ctx)\n",
    "val_label = nd.array(y_val, ctx)\n",
    "test_featrues = nd.array(test_inputs, ctx=ctx)\n",
    "# test_label = nd.array(test_label, ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\gluon\\lib\\site-packages\\mxnet\\contrib\\text\\embedding.py:278: UserWarning: At line 1 of the pre-trained text embedding file: token 985667 with 1-dimensional vector [300.0] is likely a header and is skipped.\n",
      "  'skipped.' % (line_num, token, elems))\n"
     ]
    }
   ],
   "source": [
    "glove_embedding = text.embedding.CustomEmbedding(pretrained_file_path='data/wiki.es.vec', vocabulary=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Block):\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.conv1 = nn.Conv1D(num_channels, kernel_size=3, padding=1, strides=strides)\n",
    "        self.conv2 = nn.Conv1D(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv1D(num_channels, kernel_size=1, strides=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm()\n",
    "        self.bn2 = nn.BatchNorm()\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        Y = nd.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return nd.relu(Y + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_outputs = 1\n",
    "lr = 0.01\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = SentimentNet(vocab, embed_size, num_hiddens, num_layers, bidirectional)\n",
    "net.initialize(init.Xavier(), ctx=ctx)\n",
    "# 设置 embedding 层的 weight 为预训练的词向量。\n",
    "net.embedding.weight.set_data(glove_embedding.idx_to_vec.as_in_context(ctx))\n",
    "# 训练中不更新词向量（net.embedding中的模型参数）。\n",
    "net.embedding.collect_params().setattr('grad_req', 'null')\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "loss = gloss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
